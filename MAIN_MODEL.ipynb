{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install orjson\n",
        "!pip install pytrends\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import orjson\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "from pytrends.request import TrendReq\n",
        "from dateutil import parser\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from functools import reduce\n",
        "from time import sleep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJKhD6MbqW-j",
        "outputId": "b242bfa1-471b-41b5-d3da-cb3c60f49d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (3.9.4)\n",
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.10/dist-packages (4.9.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.31.0)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (1.5.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends) (4.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.23.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.25->pytrends) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh6Sd5FbqKFg"
      },
      "outputs": [],
      "source": [
        "def get_creation(wikipedia_link, session):\n",
        "  title = wikipedia_link.split('/')[-1]\n",
        "  revisions_url = f'https://en.wikipedia.org/w/api.php?action=query&format=json&titles={title}&prop=revisions&rvprop=timestamp&rvlimit=1&rvdir=newer'\n",
        "  response = session.get(revisions_url, headers={'User-Agent':'Mozilla/5.0'})\n",
        "  data = response.json()\n",
        "  page_id = list(data['query']['pages'].keys())[0]\n",
        "  revision_date_str = data['query']['pages'][page_id]['revisions'][0]['timestamp']\n",
        "  revision_date_obj = datetime.datetime.strptime(revision_date_str, '%Y-%m-%dT%H:%M:%SZ')\n",
        "  return revision_date_obj.strftime('%Y%m%d')\n",
        "\n",
        "def get_release(wikipedia_link, session):\n",
        "  try:\n",
        "    response = session.get(wikipedia_link, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    page_content = response.text\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    table = soup.find('table', class_='infobox vevent')\n",
        "    rows = soup.find_all('tr')\n",
        "\n",
        "    for row in rows:\n",
        "      if \"Release dates\" in row.text:\n",
        "        listy = row.text.strip().split('\\n')\n",
        "        for item in listy:\n",
        "          if 'United States' in item or 'US' in item:\n",
        "            result = re.sub(r'\\([^)]*\\)', '', item)\n",
        "            result = result.replace('\\xa0', ' ').strip()\n",
        "            parsed_date = parser.parse(result)\n",
        "            return parsed_date.strftime('%Y%m%d')\n",
        "        for item in listy:\n",
        "          if '(' in item:\n",
        "            result = re.sub(r'\\([^)]*\\)', '', item)\n",
        "            result = result.replace('\\xa0', ' ').strip()\n",
        "            parsed_date = parser.parse(result)\n",
        "            return parsed_date.strftime('%Y%m%d')\n",
        "\n",
        "      elif 'Release date' in row.text:\n",
        "        listy = row.text.strip().split('\\n')\n",
        "        for item in listy:\n",
        "          if '(' in item:\n",
        "            result = re.sub(r'\\([^)]*\\)', '', item)\n",
        "            result = result.replace('\\xa0', ' ').strip()\n",
        "            parsed_date = parser.parse(result)\n",
        "            return parsed_date.strftime('%Y%m%d')\n",
        "  except:\n",
        "    return None\n",
        "\n",
        "def api_call(url, first_date, end_date, session):\n",
        "    end_part = url.split('/')[-1]\n",
        "    api = f'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/{end_part}/daily/{first_date}/{end_date}'\n",
        "    resp = session.get(api, headers={'User-Agent':'Mozilla/5.0'})\n",
        "    json_data = orjson.loads(resp.content)\n",
        "\n",
        "    start = datetime.datetime.strptime(first_date, '%Y%m%d')\n",
        "    stop = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
        "\n",
        "    views_dict = {item['timestamp'][:8]: item['views'] for item in json_data['items']}\n",
        "\n",
        "    current_date = start\n",
        "    j_data = []\n",
        "    while current_date <= stop:\n",
        "        j_data.append(views_dict.get(current_date.strftime('%Y%m%d'), 0))\n",
        "        current_date += datetime.timedelta(days=1)\n",
        "\n",
        "    return j_data\n",
        "\n",
        "def get_wiki(title, link, session, timestamp):\n",
        "  a = {'title': title}\n",
        "  a['creation'] = get_creation(link, session)\n",
        "  a['release'] = get_release(link, session)\n",
        "  a['current'] = datetime.datetime.now().date()\n",
        "  if (a['release'] is not None) and (a['current'] != (a['release'] - datetime.timedelta(timestamp))):\n",
        "    warnings.warn('Current date threshold differs from release date shown by Wikipedia. Models are calibrated for 90 days, 60 days and 30 days prior to release, and results may vary for use at other times.')\n",
        "  a['wiki_data'] = api_call(link, a['creation'], a['current'].strftime('%Y%m%d'), session)\n",
        "\n",
        "  if timestamp == '30':\n",
        "    if len(a['wiki_data']) > 90:\n",
        "      a['wiki_90_pure'] = a['wiki_data'][-90:-60]\n",
        "      a['wiki_90_raw'] = a['wiki_data'][:-60]\n",
        "      a['wiki_60_pure'] = a['wiki_data'][-60:-30]\n",
        "      a['wiki_60_raw'] = a['wiki_data'][:-30]\n",
        "      a['wiki_30_pure'] = a['wiki_data'][-30:]\n",
        "      a['wiki_30_raw'] = a['wiki_data']\n",
        "    elif len(a['wiki_data']) > 60:\n",
        "      a['wiki_90_pure'] = None\n",
        "      a['wiki_90_raw'] = a['wiki_data'][:-60]\n",
        "      a['wiki_60_pure'] = a['wiki_data'][-60:-30]\n",
        "      a['wiki_60_raw'] = a['wiki_data'][:-30]\n",
        "      a['wiki_30_pure'] = a['wiki_data'][-30:]\n",
        "      a['wiki_30_raw'] = a['wiki_data']\n",
        "    elif len(a['wiki_data']) > 30:\n",
        "      a['wiki_90_pure'] = None\n",
        "      a['wiki_90_raw'] = None\n",
        "      a['wiki_60_pure'] = None\n",
        "      a['wiki_60_raw'] = a['wiki_data'][:-30]\n",
        "      a['wiki_30_pure'] = a['wiki_data'][-30:]\n",
        "      a['wiki_30_raw'] = a['wiki_data']\n",
        "    else:\n",
        "      a['wiki_90_pure'] = None\n",
        "      a['wiki_90_raw'] = None\n",
        "      a['wiki_60_pure'] = None\n",
        "      a['wiki_60_raw'] = None\n",
        "      a['wiki_30_pure'] = None\n",
        "      a['wiki_30_raw'] = a['wiki_data']\n",
        "\n",
        "  elif timestamp == '60':\n",
        "    if len(a['wiki_data']) > 60:\n",
        "      a['wiki_90_pure'] = a['wiki_data'][-60:-30]\n",
        "      a['wiki_90_raw'] = a['wiki_data'][:-30]\n",
        "      a['wiki_60_pure'] = a['wiki_data'][-30:]\n",
        "      a['wiki_60_raw'] = a['wiki_data']\n",
        "    elif len(a['wiki_data']) > 30:\n",
        "      a['wiki_90_pure'] = None\n",
        "      a['wiki_90_raw'] = a['wiki_data'][:-30]\n",
        "      a['wiki_60_pure'] = a['wiki_data'][-30:]\n",
        "      a['wiki_60_raw'] = a['wiki_data']\n",
        "    else:\n",
        "      a['wiki_90_pure'] = None\n",
        "      a['wiki_90_raw'] = None\n",
        "      a['wiki_60_pure'] = None\n",
        "      a['wiki_60_raw'] = a['wiki_data']\n",
        "\n",
        "  elif timestamp == '90':\n",
        "    if len(a['wiki_data']) > 30:\n",
        "      a['wiki_90_pure'] = a['wiki_data'][-30:]\n",
        "      a['wiki_90_raw'] = a['wiki_data']\n",
        "    else:\n",
        "      a['wiki_90_pure'] = None\n",
        "      a['wiki_90_raw'] = a['wiki_data']\n",
        "\n",
        "  else:\n",
        "    raise ValueError('Not valid timestamp entry.')\n",
        "\n",
        "  return a\n",
        "\n",
        "# Google Trends\n",
        "\n",
        "def ready_title(text):\n",
        "  title = title.strip()\n",
        "  if len(title) >= 100:\n",
        "    film_title = film_title[:99]\n",
        "    a = film_title.rfind(':')\n",
        "    if a == -1:\n",
        "      return film_title[:a]\n",
        "    else:\n",
        "      return film_title[:a]\n",
        "\n",
        "def get_trends_data(film_title, pytrends, start_time, end_time):\n",
        "  # must be in %Y-%m-%d format\n",
        "  pytrends.build_payload(kw_list=[film_title], timeframe=f'{start_time} {end_time}')\n",
        "  trends_data = pytrends.interest_over_time()\n",
        "  trends_data = trends_data.rename(columns={film_title: 'film'})\n",
        "  return trends_data\n",
        "\n",
        "def get_gt(title, t, start_time, end_time):\n",
        "  king = True\n",
        "  while king:\n",
        "    try:\n",
        "        a = get_trends_data(title, t, start_time, end_time)\n",
        "        sleep(1)\n",
        "\n",
        "        try:\n",
        "          return a['film'].to_list()\n",
        "        except KeyError as e:\n",
        "          search_text = title\n",
        "          index = search_text.rfind(':')\n",
        "          if index == -1:\n",
        "            title = search_text[: len(search_text) // 2]\n",
        "          else:\n",
        "            title = search_text[:index]\n",
        "    except Exception as e:\n",
        "      print(e,  ': rate limit - relaunching Google Trends')\n",
        "      sleep(60)\n",
        "      continue\n",
        "\n",
        "def gt(release, title, t, timestamp):\n",
        "  current = datetime.datetime.now().days()\n",
        "  current_str = current.strftime('%Y-%m-%d')\n",
        "  back_30 = datetime.timedelta(days = 30).strftime('%Y-%m-%d')\n",
        "  back_60 = datetime.timedelta(days = 60).strftime('%Y-%m-%d')\n",
        "  back_90 = datetime.timedelta(days = 90).strftime('%Y-%m-%d')\n",
        "  title = ready_title(title)\n",
        "\n",
        "  if (release is not None) and (current != release - datetime.timedelta(timestamp)):\n",
        "    warnings.warn('Current date threshold differs from release date shown by Wikipedia. Models are calibrated for 90 days, 60 days and 30 days prior to release, and results may vary for use at other times.')\n",
        "\n",
        "  if timestamp == 30:\n",
        "    a = {'gt_90': get_gt(title, t, back_90, back_60),\n",
        "        'gt_60_pure': get_gt(title, t, back_60, back_30),\n",
        "        'gt_60_raw': get_gt(title, t, back_90, back_30),\n",
        "        'gt_30_pure': get_gt(title, t, back_30, current),\n",
        "        'gt_30_raw': get_gt(title, t, back_90, current)}\n",
        "  elif timestamp == 60:\n",
        "    a = {'gt_90': get_gt(title, t, back_60, back_30),\n",
        "        'gt_60_pure': get_gt(title, t, back_30, current),\n",
        "        'gt_60_raw': get_gt(title, t, back_60, back_30)}\n",
        "  elif timestamp == 90:\n",
        "    a = {'gt_90': get_gt(title, t, back_90, back_60)}\n",
        "  else:\n",
        "    raise ValueError('Not valid timestamp entry.')\n",
        "\n",
        "  a['title'] = title\n",
        "  return a\n",
        "\n",
        "def individual_pull(title, wikipedia_url, timestamp):\n",
        "  s = requests.Session()\n",
        "  t = TrendReq()\n",
        "\n",
        "  wiki_dict = get_wiki(title, wikipedia_url, s, timestamp)\n",
        "  gt_dict = gt(wiki_dict['release'], title, t, timestamp)\n",
        "  df_wiki = pd.DataFrame(wiki_dict)\n",
        "  df_gt = pd.DataFrame(gt_dict)\n",
        "\n",
        "  master_df = pd.merge(df_wiki, df_gt, how='inner', on='title').drop(columns=['release', 'link'])\n",
        "\n",
        "  return master_df\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib3.util.retry import RequestHistory\n",
        "def filter_it(subset):\n",
        "  if len(subset) > 90:\n",
        "    a = {}\n",
        "    a['wiki_earliest_set_pure'] = subset[-90:-60]\n",
        "    a['wiki_earliest_set_com'] = subset[-90:]\n",
        "    a['wiki_mid_set_pure'] = subset[-60:-30]\n",
        "    a['wiki_mid_set_com'] = subset[-60:]\n",
        "    a['wiki_latest_set'] = subset[-30:]\n",
        "    a['wiki_all_data'] = subset\n",
        "    return a\n",
        "  elif len(subset) > 60:\n",
        "    a = {}\n",
        "    a['wiki_earliest_set_pure'] = None\n",
        "    a['wiki_earliest_set_com'] = subset[-90:]\n",
        "    a['wiki_mid_set_pure'] = subset[-60:-30]\n",
        "    a['wiki_mid_set_com'] = subset[-60:]\n",
        "    a['wiki_latest_set'] = subset[-30:]\n",
        "    a['wiki_all_data'] = subset\n",
        "  elif len(subset) > 30:\n",
        "    a = {}\n",
        "    a['wiki_earliest_set_pure'] = None\n",
        "    a['wiki_earliest_set_com'] = None\n",
        "    a['wiki_mid_set_pure'] = None\n",
        "    a['wiki_mid_set_com'] = subset[-60:]\n",
        "    a['wiki_latest_set'] = subset[-30:]\n",
        "    a['wiki_all_data'] = subset\n",
        "  else:\n",
        "    a = {}\n",
        "    a['wiki_earliest_set_pure'] = None\n",
        "    a['wiki_earliest_set_com'] = None\n",
        "    a['wiki_mid_set_pure'] = None\n",
        "    a['wiki_mid_set_com'] = None\n",
        "    a['wiki_latest_set'] = None\n",
        "    a['wiki_all_data'] = subset\n",
        "  return a\n",
        "\n",
        "def get_wiki(link, session, timestamp):\n",
        "  release = get_release(link, session)\n",
        "  creation = get_creation(link, session)\n",
        "  data = api_call(link, creation, release, session)\n",
        "  if timestamp == 90:\n",
        "    x = filter_it(data[:-90])\n",
        "  elif timestamp == 60:\n",
        "    x = filter_it(data[:-60])\n",
        "  elif timestamp == 30:\n",
        "    x = filter_it(data[:-30])\n",
        "  else:\n",
        "    raise ValueError('Not valid timestamp entry.')\n",
        "\n",
        "  x['wikipedia_url'] = link\n",
        "  x['release'] = release\n",
        "  return x\n",
        "\n",
        "def ready_title(text):\n",
        "  title = re.sub(r'\\([^)]*\\)', '', text)\n",
        "  title = title.strip()\n",
        "  if len(title) >= 100:\n",
        "    film_title = film_title[:99]\n",
        "    a = film_title.rfind(':')\n",
        "    if a == -1:\n",
        "      return film_title[:a]\n",
        "    else:\n",
        "      return film_title[:a]\n",
        "  else:\n",
        "    return title\n",
        "\n",
        "def get_trends_data(film_title, release, t, timestamp):\n",
        "  a = {'title', film_title}\n",
        "  film_title = ready_title(film_title)\n",
        "  release = datetime.datetime.strptime(release, \"%Y%m%d\")\n",
        "  end = release - pd.DateOffset(days = timestamp)\n",
        "\n",
        "  times = {\n",
        "      'gt_earliest_set_pure': (90, 0),\n",
        "      'gt_earliest_set_raw': (90, 60),\n",
        "      'gt_mid_set_pure': (60, 30),\n",
        "      'gt_mid_set_com': (60, 0),\n",
        "      'gt_latest_set': (30,0)\n",
        "      }\n",
        "\n",
        "  for key, item in times:\n",
        "    start_date = end - datetime.timedelta(item[0]).strftime('%Y-%m-%d')\n",
        "    end_date = end - datetime.timedelta(item[1]).strftime('%Y-%m-%d')\n",
        "    retry = True\n",
        "    while retry:\n",
        "      try:\n",
        "        t.build_payload(kw_list=[film_title], timeframe=f'{start_date} {end_date}')\n",
        "        trends_data = t.interest_over_time()\n",
        "\n",
        "        try:\n",
        "          a[key] = trends_data['film'].to_list()\n",
        "          sleep(1)\n",
        "          retry = False\n",
        "\n",
        "        except KeyError:\n",
        "          index = film_title.rfind(':')\n",
        "          if index == -1:\n",
        "            film_title = film_title[: len(film_title) // 2]\n",
        "          else:\n",
        "            film_title = film_title[:film_title]\n",
        "\n",
        "      except Exception as e:\n",
        "        print(e,  ': rate limit hit - relaunching pull for', film_title)\n",
        "        sleep(60)\n",
        "\n",
        "  return a"
      ],
      "metadata": {
        "id": "VN_dm3KtbOOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def master_data(df, timeframe):\n",
        "  s = requests.Session()\n",
        "  t = TrendReq()\n",
        "\n",
        "  tqdm.pandas(desc='{Wikipedia Data Pull}')\n",
        "  df['wiki_data'] = df['wikipedia_url'].progress_apply(lambda x: get_wiki(x, s, timeframe))\n",
        "  wiki_df = pd.json_normalize(df['wiki_data'])\n",
        "  df = pd.merge(df, wiki_df, how='inner', on='wikipedia_url').drop(columns=['wiki_data'])\n",
        "\n",
        "  tqdm.pandas(desc='{Google Trends Pull - may take multiple iterations}')\n",
        "  df['gt_data'] = df.progress_apply(lambda x: get_trends_data(x['title'], x['release'], t, timeframe), axis=1)\n",
        "  gt_df = pd.json_normalize(df['gt_data'])\n",
        "  df = pd.merge(df, gt_df, how='inner', on='title').drop(columns=['gt_data'])\n",
        "\n",
        "  return df\n",
        "\n",
        "df = pd.read_csv('input.csv')\n",
        "df2 = df[['title', 'wikipedia_url']]\n",
        "\n",
        "a = master_data(df2, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "lmeFvTWAkFey",
        "outputId": "c5295692-7539-4a73-94de-af7d09653766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{Wikipedia Data Pull}: 100%|██████████| 104/104 [00:22<00:00,  4.55it/s]\n",
            "<ipython-input-42-8836c4ca645f>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['wiki_data'] = df['wikipedia_url'].progress_apply(lambda x: get_wiki(x, s, timeframe))\n",
            "{Google Trends Pull - may take multiple iterations}:   1%|          | 1/164 [00:00<00:00, 995.80it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-8836c4ca645f>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wikipedia_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaster_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-8836c4ca645f>\u001b[0m in \u001b[0;36mmaster_data\u001b[0;34m(df, timeframe)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{Google Trends Pull - may take multiple iterations}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_trends_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'release'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mgt_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m                 \u001b[0;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9566\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9567\u001b[0m         )\n\u001b[0;32m-> 9568\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9570\u001b[0m     def applymap(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    798\u001b[0m                     \u001b[0;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-8836c4ca645f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{Google Trends Pull - may take multiple iterations}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_trends_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'release'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mgt_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-b2591624f85b>\u001b[0m in \u001b[0;36mget_trends_data\u001b[0;34m(film_title, release, t, timestamp)\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilm_title\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0mfilm_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mready_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilm_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   \u001b[0mrelease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%Y%m%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m   \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelease\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDateOffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'strptime'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/linksteset.csv')\n",
        "df2 = pd.read_csv('/content/okok.csv')['title']\n",
        "df3 = pd.merge(df1, df2, how='inner', on = 'title')\n",
        "df3.loc[86, 'title'] = 'Big George Foreman (2023)'\n",
        "df3 = df3[['title', 'wikipedia_url']]\n",
        "#df4 = pd.read_csv('/content/Box Office - Sheet2.csv')\n",
        "#df5 = pd.merge(df3, df4, on='title', how='inner')\n",
        "\n",
        "df5.to_csv('input.csv')"
      ],
      "metadata": {
        "id": "WR3nv3q6jyoT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}