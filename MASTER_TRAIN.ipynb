{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports - DO NOT TOUCH, just hit run all (play button)"
      ],
      "metadata": {
        "id": "vv2HY0ML5VjE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYVhUg7G5A0T",
        "outputId": "41080ceb-f355-4eb8-f1bd-9d9b919f47fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting orjson\n",
            "  Downloading orjson-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/140.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/140.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.3/140.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: orjson\n",
            "Successfully installed orjson-3.9.4\n",
            "Collecting pytrends\n",
            "  Downloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.31.0)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (1.5.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends) (4.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.23.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.25->pytrends) (1.16.0)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install orjson\n",
        "!pip install pytrends\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import orjson\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "from pytrends.request import TrendReq\n",
        "from dateutil import parser\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from functools import reduce\n",
        "from time import sleep\n",
        "import ast\n",
        "import statistics as s\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import zscore\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
        "import joblib\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection - DO NOT TOUCH, just hit run all (play button)"
      ],
      "metadata": {
        "id": "r9YiZ1_75ZEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box Office\n",
        "\n",
        "# Box Office: get_box_office(df, session) -> df, join on 'url'\n",
        "\n",
        "def scrape_box(url, session):\n",
        "    r = session.get(url, headers={'User-Agent':'Mozilla/5.0'})\n",
        "    soup = BeautifulSoup(r.text, 'lxml')\n",
        "    a = soup.find('table', id='movie_finances')\n",
        "    pop = a.find_all('tr', class_='heading')\n",
        "    if pop != []:\n",
        "      for x in pop:\n",
        "        x.extract()\n",
        "\n",
        "    fin = a.find_all('td')\n",
        "\n",
        "    b = soup.find('div', class_='card-body')\n",
        "    c = b.find_all('table')\n",
        "    d = c[1]\n",
        "    rows = d.find_all('td')\n",
        "    all = fin + rows\n",
        "\n",
        "    p = {'Opening\\xa0Weekend:' : 'opening',\n",
        "         'Legs:' : 'legs',\n",
        "         'Domestic Share:' : 'share',\n",
        "         'Production\\xa0Budget:' : 'prod',\n",
        "         'Theater counts:' : 'theater',\n",
        "         'Infl. Adj. Dom. BO' : 'adjusted',\n",
        "         'Domestic Box Office': 'domestic_total',\n",
        "         'International Box Office' : 'international_total',\n",
        "         'Worldwide Box Office' : 'all_total',\n",
        "         'Est. Domestic DVD Sales' : 'domestic_dvd',\n",
        "         'Est. Domestic Blu-ray Sales' : 'domestic_bluray',\n",
        "         'Total Est. Domestic Video Sales': 'domestic_home'\n",
        "         }\n",
        "\n",
        "    dicto = {\n",
        "      'url': url,\n",
        "      'official_name': soup.find('div', id='main').find('h1').text,\n",
        "      'opening': None,\n",
        "      'share': None,\n",
        "      'legs': None,\n",
        "      'prod':  None,\n",
        "      'theater': None,\n",
        "      'adjusted': None,\n",
        "      'domestic_total': None,\n",
        "      'international_total': None,\n",
        "      'all_total': None,\n",
        "      'domestic_dvd': None,\n",
        "      'domestic_bluray': None,\n",
        "      'domestic_home': None\n",
        "    }\n",
        "\n",
        "    for x in range(len(all)):\n",
        "      iter = all[x].text\n",
        "      if iter in p.keys():\n",
        "        dicto[p[iter]] = all[x+1].text\n",
        "\n",
        "    return dicto\n",
        "\n",
        "def cut(text):\n",
        "  if text is not None:\n",
        "    text = str(text)\n",
        "    pattern = r'\\([^)]*\\)'\n",
        "    cleaned_text = re.sub(pattern, '', text)\n",
        "    return cleaned_text\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def extract(text):\n",
        "  text = str(text)\n",
        "  pattern = r'\\((.*?)\\)'\n",
        "  match = re.search(pattern, text)\n",
        "  if match:\n",
        "      return match.group(1)\n",
        "  return None\n",
        "\n",
        "def number(text):\n",
        "  pattern = r'\\d+(\\.\\d+)?'\n",
        "  text = str(text)\n",
        "  match = re.search(pattern, text)\n",
        "  if match:\n",
        "      number = match.group()\n",
        "      return float(number) if '.' in number else int(number)\n",
        "  return None\n",
        "\n",
        "def theatre(text):\n",
        "  text = str(text)\n",
        "  pattern = r'\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b'\n",
        "  numbers_with_commas = re.findall(pattern, text)\n",
        "  numbers = [float(num.replace(',', '')) if '.' in num else int(num.replace(',', '')) for num in numbers_with_commas]\n",
        "  return numbers\n",
        "\n",
        "def format(text):\n",
        "  if text is None:\n",
        "    return None\n",
        "  elif '$' in text:\n",
        "    text = text.replace('$', '')\n",
        "    if ',' in text:\n",
        "      text = text.replace(',', '')\n",
        "    return float(text)\n",
        "  elif 'n/a' in text:\n",
        "    return None\n",
        "\n",
        "def alter_df(df1):\n",
        "  for x in ['share', 'legs', 'opening', 'prod']:\n",
        "    df1[x] = df1[x].apply(cut)\n",
        "\n",
        "  df1['opening_percentage'] = df1['opening'].apply(extract)\n",
        "  df1['prod_multiple'] = df1['prod'].apply(extract)\n",
        "\n",
        "  for x in ['prod_multiple', 'opening_percentage', 'share']:\n",
        "    df1[x] = df1.apply(number)\n",
        "\n",
        "  df1['theater'] = df1['theater'].apply(theatre)\n",
        "  df1['opening_theater'] = df1['theater'].apply(lambda x: None if len(x) < 3 else x[0])\n",
        "  df1['max_theater'] = df1['theater'].apply(lambda x: None if len(x) < 3 else x[1])\n",
        "  df1['average_theater_run'] = df1['theater'].apply(lambda x: None if len(x) < 3 else x[2])\n",
        "  df1 = df1.drop('theater', axis=1)\n",
        "\n",
        "  for x in ['opening', 'prod', 'domestic_total', 'international_total', 'all_total', 'domestic_dvd', 'domestic_bluray', 'domestic_home']:\n",
        "    df1[x] = df1[x].apply(format)\n",
        "\n",
        "  return df1\n",
        "\n",
        "def get_box_office(df, session):\n",
        "  tqdm.pandas(desc='{Box Office Pull}')\n",
        "  df['data'] = df['the_numbers_url'].progress_apply(lambda x: scrape_box(x, session))\n",
        "  un_df = pd.json_normalize(df['data'])\n",
        "  formatted = alter_df(un_df)\n",
        "  formatted = formatted['title', 'opening', 'domestic_adjusted', 'domestic_total', 'international_total', 'international_adjusted', 'all_total', 'all_adjusted', 'domestic_with_home']\n",
        "  return formatted"
      ],
      "metadata": {
        "id": "rHJQd3Nz01RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wikipedia\n",
        "\n",
        "def filter_it(subset):\n",
        "  if len(subset) > 90:\n",
        "    a = {}\n",
        "    a['wiki_earliest_set_pure'] = subset[-90:-60]\n",
        "    a['wiki_earliest_set_com'] = subset[-90:]\n",
        "    a['wiki_mid_set_pure'] = subset[-60:-30]\n",
        "    a['wiki_mid_set_com'] = subset[-60:]\n",
        "    a['wiki_latest_set'] = subset[-30:]\n",
        "    a['wiki_all_data'] = subset\n",
        "    return a\n",
        "  elif len(subset) > 60:\n",
        "    a = {}\n",
        "    a['wiki_earliest_set_pure'] = None\n",
        "    a['wiki_earliest_set_com'] = subset[-90:]\n",
        "    a['wiki_mid_set_pure'] = subset[-60:-30]\n",
        "    a['wiki_mid_set_com'] = subset[-60:]\n",
        "    a['wiki_latest_set'] = subset[-30:]\n",
        "    a['wiki_all_data'] = subset\n",
        "  elif len(subset) > 30:\n",
        "    a = {}\n",
        "    a['wiki_earliest_set_pure'] = None\n",
        "    a['wiki_earliest_set_com'] = None\n",
        "    a['wiki_mid_set_pure'] = None\n",
        "    a['wiki_mid_set_com'] = subset[-60:]\n",
        "    a['wiki_latest_set'] = subset[-30:]\n",
        "    a['wiki_all_data'] = subset\n",
        "  else:\n",
        "    a = {}\n",
        "    a['wiki_earliest_set_pure'] = None\n",
        "    a['wiki_earliest_set_com'] = None\n",
        "    a['wiki_mid_set_pure'] = None\n",
        "    a['wiki_mid_set_com'] = None\n",
        "    a['wiki_latest_set'] = None\n",
        "    a['wiki_all_data'] = subset\n",
        "  return a\n",
        "\n",
        "def get_creation(wikipedia_link, session):\n",
        "  title = wikipedia_link.split('/')[-1]\n",
        "  revisions_url = f'https://en.wikipedia.org/w/api.php?action=query&format=json&titles={title}&prop=revisions&rvprop=timestamp&rvlimit=1&rvdir=newer'\n",
        "  response = session.get(revisions_url, headers={'User-Agent':'Mozilla/5.0'})\n",
        "  data = response.json()\n",
        "  page_id = list(data['query']['pages'].keys())[0]\n",
        "  revision_date_str = data['query']['pages'][page_id]['revisions'][0]['timestamp']\n",
        "  revision_date_obj = datetime.datetime.strptime(revision_date_str, '%Y-%m-%dT%H:%M:%SZ')\n",
        "  return revision_date_obj.strftime('%Y%m%d')\n",
        "\n",
        "def get_release(wikipedia_link, session):\n",
        "  try:\n",
        "    response = session.get(wikipedia_link, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    page_content = response.text\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    table = soup.find('table', class_='infobox vevent')\n",
        "    rows = soup.find_all('tr')\n",
        "\n",
        "    for row in rows:\n",
        "      if \"Release dates\" in row.text:\n",
        "        listy = row.text.strip().split('\\n')\n",
        "        for item in listy:\n",
        "          if 'United States' in item or 'US' in item:\n",
        "            result = re.sub(r'\\([^)]*\\)', '', item)\n",
        "            result = result.replace('\\xa0', ' ').strip()\n",
        "            parsed_date = parser.parse(result)\n",
        "            return parsed_date.strftime('%Y%m%d')\n",
        "        for item in listy:\n",
        "          if '(' in item:\n",
        "            result = re.sub(r'\\([^)]*\\)', '', item)\n",
        "            result = result.replace('\\xa0', ' ').strip()\n",
        "            parsed_date = parser.parse(result)\n",
        "            return parsed_date.strftime('%Y%m%d')\n",
        "\n",
        "      elif 'Release date' in row.text:\n",
        "        listy = row.text.strip().split('\\n')\n",
        "        for item in listy:\n",
        "          if '(' in item:\n",
        "            result = re.sub(r'\\([^)]*\\)', '', item)\n",
        "            result = result.replace('\\xa0', ' ').strip()\n",
        "            parsed_date = parser.parse(result)\n",
        "            return parsed_date.strftime('%Y%m%d')\n",
        "  except:\n",
        "    return None\n",
        "\n",
        "def api_call(url, first_date, end_date, session):\n",
        "    end_part = url.split('/')[-1]\n",
        "    api = f'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/{end_part}/daily/{first_date}/{end_date}'\n",
        "    resp = session.get(api, headers={'User-Agent':'Mozilla/5.0'})\n",
        "    json_data = orjson.loads(resp.content)\n",
        "\n",
        "    start = datetime.datetime.strptime(first_date, '%Y%m%d')\n",
        "    stop = datetime.datetime.strptime(end_date, '%Y%m%d')\n",
        "\n",
        "    views_dict = {item['timestamp'][:8]: item['views'] for item in json_data['items']}\n",
        "\n",
        "    current_date = start\n",
        "    j_data = []\n",
        "    while current_date <= stop:\n",
        "        j_data.append(views_dict.get(current_date.strftime('%Y%m%d'), 0))\n",
        "        current_date += datetime.timedelta(days=1)\n",
        "\n",
        "    return j_data\n",
        "\n",
        "def get_wiki(link, session, timestamp):\n",
        "  release = get_release(link, session)\n",
        "  creation = get_creation(link, session)\n",
        "  data = api_call(link, creation, release, session)\n",
        "  if timestamp == 90:\n",
        "    x = filter_it(data[:-90])\n",
        "  elif timestamp == 60:\n",
        "    x = filter_it(data[:-60])\n",
        "  elif timestamp == 30:\n",
        "    x = filter_it(data[:-30])\n",
        "  else:\n",
        "    raise ValueError('Not valid timestamp entry.')\n",
        "\n",
        "  x['wikipedia_url'] = link\n",
        "  x['release'] = release\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "4jykX0aY5Psf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ready_title(text):\n",
        "  title = re.sub(r'\\([^)]*\\)', '', text)\n",
        "  title = title.strip()\n",
        "  if len(title) >= 100:\n",
        "    film_title = film_title[:99]\n",
        "    a = film_title.rfind(':')\n",
        "    if a == -1:\n",
        "      return film_title[:a]\n",
        "    else:\n",
        "      return film_title[:a]\n",
        "  else:\n",
        "    return title\n",
        "\n",
        "def get_trends_data(film_title, release, t, timestamp):\n",
        "  a = {'title': film_title}\n",
        "  film_title = ready_title(film_title)\n",
        "  release = datetime.datetime.strptime(release, \"%Y%m%d\")\n",
        "  end = release - pd.DateOffset(days = timestamp)\n",
        "\n",
        "  times = {\n",
        "      'gt_earliest_set_pure': (90, 0),\n",
        "      'gt_earliest_set_raw': (90, 60),\n",
        "      'gt_mid_set_pure': (60, 30),\n",
        "      'gt_mid_set_com': (60, 0),\n",
        "      'gt_latest_set': (30,0)\n",
        "    }\n",
        "\n",
        "  for key, item in tqdm(times.items(), desc=f'Google Trend Pulls for {film_title}'):\n",
        "    start_date = (end - datetime.timedelta(item[0])).strftime('%Y-%m-%d')\n",
        "    end_date = (end - datetime.timedelta(item[1])).strftime('%Y-%m-%d')\n",
        "    retry = True\n",
        "    while retry:\n",
        "      try:\n",
        "        t.build_payload(kw_list=[film_title], timeframe=f'{start_date} {end_date}')\n",
        "        trends_data = t.interest_over_time()\n",
        "        trends_data = trends_data.rename(columns={film_title: 'film'})\n",
        "\n",
        "        try:\n",
        "          a[key] = trends_data['film'].to_list()\n",
        "          sleep(1)\n",
        "          break\n",
        "\n",
        "        except KeyError as e:\n",
        "          print(e)\n",
        "          index = film_title.rfind(':')\n",
        "          if index == -1:\n",
        "            film_title = film_title[: len(film_title) // 2]\n",
        "          else:\n",
        "            film_title = film_title[:index]\n",
        "\n",
        "      except Exception as e:\n",
        "        print(e,  ': rate limit hit - relaunching pull for', film_title)\n",
        "        sleep(60)\n",
        "\n",
        "  return a"
      ],
      "metadata": {
        "id": "EmOIDi5q5UmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def master_data(df, timeframe):\n",
        "  s = requests.Session()\n",
        "  t = TrendReq()\n",
        "\n",
        "  tqdm.pandas(desc='{Wikipedia Data Pull}')\n",
        "  df['wiki_data'] = df['wikipedia_url'].progress_apply(lambda x: get_wiki(x, s, timeframe))\n",
        "  wiki_df = pd.json_normalize(df['wiki_data'])\n",
        "  df = pd.merge(df, wiki_df, how='inner', on='wikipedia_url').drop(columns=['wiki_data'])\n",
        "\n",
        "  tqdm.pandas(desc='{Google Trends Pull - may take multiple iterations}')\n",
        "  df['gt_data'] = df.progress_apply(lambda x: get_trends_data(x['title'], x['release'], t, timeframe), axis=1)\n",
        "  gt_df = pd.json_normalize(df['gt_data'])\n",
        "  df = pd.merge(df, gt_df, how='inner', on='title').drop(columns=['gt_data'])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "jnTUqN825q_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis - DO NOT TOUCH, just hit run all (play button)"
      ],
      "metadata": {
        "id": "2KCOzPDsTJNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical analysis\n",
        "\n",
        "def normalize_list(input_list):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 100))\n",
        "    normalized_list = scaler.fit_transform([[x] for x in input_list])\n",
        "    return normalized_list.ravel().tolist()\n",
        "\n",
        "def temp_solution(text):\n",
        "  try:\n",
        "    return ast.literal_eval(str(text))\n",
        "  except ValueError:\n",
        "    if str(text).lower() == \"nan\":\n",
        "      return None  # or you can return \"nan\" or any other placeholder value\n",
        "    else:\n",
        "      raise\n",
        "\n",
        "def try_std(x):\n",
        "  try:\n",
        "    return s.stdev(x) if x is not None else None\n",
        "  except ValueError:\n",
        "    return None\n",
        "\n",
        "def analyze(df):\n",
        "  df = df.copy()\n",
        "\n",
        "  transformations = [('sum', lambda x: sum(x)  if x is not None else None),\n",
        "                     ('avg', lambda x: s.mean(x) if x is not None else None),\n",
        "                     ('std', try_std),\n",
        "                      #lambda x: s.stdev(x) if x is not None else None),\n",
        "                     ('median', lambda x: s.median(x)  if x is not None else None),\n",
        "                     ('mode', lambda x: s.mode(x)  if x is not None else None),\n",
        "                     ('range', lambda x: max(x) - min(x)  if x is not None else None),\n",
        "                     ('iqr', lambda x:  s.quantiles(x, n=4)[-1] - (s.quantiles(x, n=4)[0])  if x is not None else None),\n",
        "                    ]\n",
        "\n",
        "  '''\n",
        "  for column in df.drop(columns=['title', 'release']):\n",
        "\n",
        "    if ('skew' not in column) and ('kurtosis' not in column):\n",
        "      for x in transformations:\n",
        "        df[column + '_' + x[0]] = df[column].apply(x[1])\n",
        "    df = df.drop(column, axis=1)\n",
        "  '''\n",
        "\n",
        "  new_cols = []\n",
        "  drop_col = []\n",
        "  for column in df.drop(columns=['title', 'release', 'date']):\n",
        "      if ('skew' not in column) and ('kurtosis' not in column):\n",
        "          temp_df = pd.DataFrame()\n",
        "          for x in transformations:\n",
        "            temp_df[column + '_' + x[0]] = df[column].apply(x[1])\n",
        "            drop_col.append(column)\n",
        "          new_cols.append(temp_df)\n",
        "\n",
        "  df = pd.concat([df, *new_cols], axis=1)\n",
        "  df = df.drop(columns = drop_col)\n",
        "\n",
        "  return df\n",
        "\n",
        "def expand(df):\n",
        "  df = df.copy()\n",
        "  df = df.drop('wikipedia_url', axis=1)\n",
        "  df['wiki_all_data'] = df['wiki_all_data'].apply(lambda x: None if x == [] else x)\n",
        "  first = df.drop(columns=['title', 'release', 'date'])\n",
        "\n",
        "  for column in first:\n",
        "    #df[column + '_skew'] = df[column].apply(lambda x: stats.skew(x) if x is not None else None)\n",
        "    #df[column + '_kurtosis'] = df[column].apply(lambda x: stats.kurtosis(x) if x is not None else None)\n",
        "    if 'wiki' in column:\n",
        "      df[column + '_norm'] = df[column].apply(lambda x: normalize_list(x) if x is not None else None)\n",
        "\n",
        "  df = analyze(df)\n",
        "\n",
        "  return df.copy()"
      ],
      "metadata": {
        "id": "o9JfyzPe6gMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning - DO NOT TOUCH, just hit run all (play button)"
      ],
      "metadata": {
        "id": "rBPqfGbATYFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_models = {\n",
        "    \"Decision Tree\": DecisionTreeRegressor(),\n",
        "    \"Extra Trees\": ExtraTreesRegressor(),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
        "    \"Random Forest\": RandomForestRegressor()\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    \"Decision Tree\": {\n",
        "        'max_depth': [None, 5, 10, 15],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2]\n",
        "    },\n",
        "\n",
        "    \"Extra Trees\": {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2]\n",
        "    },\n",
        "\n",
        "    \"Gradient Boosting\": {\n",
        "        'n_estimators': [50, 100],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2],\n",
        "        'subsample': [0.8, 1.0]\n",
        "    },\n",
        "\n",
        "    \"Random Forest\": {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2]\n",
        "    }\n",
        "}\n",
        "\n",
        "def best_predictive_models(input_df, output_df):\n",
        "  \"\"\"Given an input and output DataFrame, returns the best models by R^2 score.\"\"\"\n",
        "\n",
        "  def preprocess_data(input_df, output_df, column):\n",
        "    \"\"\"Prepare data for training.\"\"\"\n",
        "    merged_df = pd.merge(input_df, output_df, left_on='title', right_on='title', how='inner')\n",
        "    merged_df.fillna(merged_df.median(numeric_only=True), inplace=True)\n",
        "    X = merged_df.drop(columns=['title', 'release'] + list(output_df.columns))\n",
        "    y = merged_df[column]\n",
        "    return X, y\n",
        "\n",
        "  def get_best_model_for_preprocessed_data(X, y, n_iter=10):\n",
        "        \"\"\"Find the best model for the preprocessed data.\"\"\"\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
        "        best_r2 = float('-inf')\n",
        "        best_model_info = None\n",
        "\n",
        "        for model_name, model_instance in ml_models.items():\n",
        "            grid_search = RandomizedSearchCV(model_instance, param_grids[model_name], n_iter=n_iter, cv=5, scoring='r2', n_jobs=-1, random_state=4)\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            best_model = grid_search.best_estimator_\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            if r2 > best_r2:\n",
        "                best_r2 = r2\n",
        "                best_model_info = {\n",
        "                    'Model': model_name,\n",
        "                    'Instance': best_model,\n",
        "                    'Best Hyperparameters': grid_search.best_params_,\n",
        "                    'R^2': r2\n",
        "                }\n",
        "        return best_model_info\n",
        "\n",
        "  # Dictionary to store the results and trained models\n",
        "  results = {}\n",
        "  trained_models = {}\n",
        "\n",
        "  for column in output_df.columns.drop(\"title\"):\n",
        "      X, y = preprocess_data(input_df, output_df, column)\n",
        "      result = get_best_model_for_preprocessed_data(X, y)\n",
        "      results[column] = f\"{result['Model']} with R^2: {result['R^2']}\"\n",
        "      # Modified the following line to store a tuple as described\n",
        "      trained_models[column] = (result['Instance'], result['R^2'], result['Best Hyperparameters'])\n",
        "      print(f\"Best Model for {column}: {results[column]}\\n\")\n",
        "\n",
        "  return trained_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yett3iP5QmQf",
        "outputId": "5baacd46-4b4d-4870-c253-ffc0f2fbb758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model for opening: Gradient Boosting with R^2: 0.7680471361032073\n",
            "\n",
            "Best Model for domestic_adjusted: Gradient Boosting with R^2: 0.38928213220986074\n",
            "\n",
            "Best Model for domestic_total: Gradient Boosting with R^2: 0.338594694140509\n",
            "\n",
            "Best Model for international_total: Random Forest with R^2: 0.8150215420261888\n",
            "\n",
            "Best Model for international_adjusted: Random Forest with R^2: 0.7389423616872677\n",
            "\n",
            "Best Model for domestic_with_home: Gradient Boosting with R^2: 0.5061498291170246\n",
            "\n",
            "Best Model for all_total: Random Forest with R^2: 0.8485818583945088\n",
            "\n",
            "Best Model for all_adjusted: Gradient Boosting with R^2: 0.7069531582042514\n",
            "\n",
            "Best Model for domestic_home: Random Forest with R^2: 0.5476382398811566\n",
            "\n",
            "Best Model for opening: Random Forest with R^2: 0.7228369900343856\n",
            "\n",
            "Best Model for domestic_adjusted: Extra Trees with R^2: 0.1360661754252257\n",
            "\n",
            "Best Model for domestic_total: Gradient Boosting with R^2: 0.07154503546113611\n",
            "\n",
            "Best Model for international_total: Extra Trees with R^2: 0.856727893244689\n",
            "\n",
            "Best Model for international_adjusted: Extra Trees with R^2: 0.8574055569808247\n",
            "\n",
            "Best Model for domestic_with_home: Extra Trees with R^2: 0.1946228399077623\n",
            "\n",
            "Best Model for all_total: Random Forest with R^2: 0.8901628311462225\n",
            "\n",
            "Best Model for all_adjusted: Random Forest with R^2: 0.8414167463752376\n",
            "\n",
            "Best Model for domestic_home: Random Forest with R^2: 0.6215770266586416\n",
            "\n",
            "Best Model for opening: Extra Trees with R^2: 0.6450881936032923\n",
            "\n",
            "Best Model for domestic_adjusted: Gradient Boosting with R^2: 0.5785613243759595\n",
            "\n",
            "Best Model for domestic_total: Gradient Boosting with R^2: 0.39923781501202926\n",
            "\n",
            "Best Model for international_total: Random Forest with R^2: 0.8773554297165174\n",
            "\n",
            "Best Model for international_adjusted: Extra Trees with R^2: 0.8954554436083422\n",
            "\n",
            "Best Model for domestic_with_home: Gradient Boosting with R^2: 0.648872311856058\n",
            "\n",
            "Best Model for all_total: Random Forest with R^2: 0.8556620756397046\n",
            "\n",
            "Best Model for all_adjusted: Random Forest with R^2: 0.895016577219748\n",
            "\n",
            "Best Model for domestic_home: Random Forest with R^2: 0.7939318927144305\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save(dictionary, metric, time):\n",
        "  filename = f'{time}_{metric}.joblib'\n",
        "  joblib.dump(dictionary[metric], '/content/' + filename)\n",
        "  return filename"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "NxV2f59Xewuj",
        "outputId": "508d75cb-dedd-49cc-d865-d87598956a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!git clone https://github.com/nathanchong25/film_ml\\n%cd film_ml\\n!mv /content/myfile.txt /content/film_ml/\\n!git config --global user.email \"nathanchong25@gmail.com\"\\n!git config --global user.name \"nathanchong25\"\\n\\nfor file in files_to_push:\\n    !git add {file}\\n    !git commit -m \"Added/Updated {file}\"\\n\\n!git push origin master\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recalibrate(df):\n",
        "  df30n = expand(master_data(df[['title', 'wikipedia_url']], 30))\n",
        "  df60n = expand(master_data(df['title', 'wikipedia_url'], 60))\n",
        "  df90n = expand(master_data(df['title', 'wikipedia_url'], 90))\n",
        "  s = requests.Session()\n",
        "  output = get_box_office(df[['title', 'the_numbers_url']])\n",
        "  a = best_predictive_models(df30n, output)\n",
        "  b = best_predictive_models(df60n, output)\n",
        "  c = best_predictive_models(df90n, output)\n",
        "  return (a, b, c)\n",
        "\n",
        "def main():\n",
        "  file_test = True\n",
        "  while file_test:\n",
        "    filepath = input('Enter filename (ensure file is uploaded in Files on the left, enter as file.csv or file.xlsx): ')\n",
        "\n",
        "    if filepath[-4:] == '.csv':\n",
        "      try:\n",
        "        df = pd.read_csv('/content/' + filepath)\n",
        "        if 'Unnamed: 0' in set(df.columns):\n",
        "          df = df.drop('Unnamed: 0', axis = 1)\n",
        "        elif '' in set(df.columns):\n",
        "          df = df.drop('', axis = 1)\n",
        "        if set(df.columns) == {'title', 'wikipedia_url', 'the_numbers_url'}:\n",
        "          file_test = False\n",
        "        else:\n",
        "          print('Table columns are incorrect - check input file is in the correct format.')\n",
        "      except FileNotFoundError:\n",
        "        print('File not found - check filename and ensure file is uploaded in the right location (content folder).')\n",
        "      except:\n",
        "        print('Problem reading input file - check input file is in the correct format.')\n",
        "\n",
        "    elif filepath[-5:] == '.xlsx':\n",
        "      try:\n",
        "        warnings.warn('Excel file must only be one sheet.')\n",
        "        df = pd.read_excel('/content/' + filepath)\n",
        "        if 'Unnamed: 0' in set(df.columns):\n",
        "          df = df.drop('Unnamed: 0', axis = 1)\n",
        "        elif '' in set(df.columns):\n",
        "          df = df.drop('', axis = 1)\n",
        "        if set(df.columns) == {'title', 'wikipedia_url', 'the_numbers_url'}:\n",
        "          file_test = False\n",
        "        else:\n",
        "          print('Table columns are incorrect - check input file is in the correct format.')\n",
        "      except FileNotFoundError:\n",
        "        print('File not found - check filename and ensure file is uploaded in the right location (content folder).')\n",
        "      except ValueError:\n",
        "        print('Problem reading input file - ensure that the Excel file is only one sheet.')\n",
        "      except:\n",
        "        print('Problem reading input file - check input file is in the correct format.')\n",
        "    else:\n",
        "       print('Faulty input - program only accepts CSV (.csv) and Excel (.xlsx) files.')\n",
        "\n",
        "    a = recalibrate(df)\n",
        "\n",
        "    global files_to_push\n",
        "    files_to_push = []\n",
        "    for x in a:\n",
        "      for y in x:\n",
        "        for z in [30, 60, 90]:\n",
        "          files_to_push.append(save(x, y, z))\n",
        "\n",
        "    return a\n"
      ],
      "metadata": {
        "id": "5nH7Ci1U2QCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Interface"
      ],
      "metadata": {
        "id": "PT3I1ooCvTRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files_to_push = []\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "3MzlMsRJt7Ze",
        "outputId": "769e6bb4-6753-441b-89b0-32efeae6289d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-18a3a37239fb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mfile_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mfile_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter filename (ensure file is uploaded in Files on the left, enter as file.csv or file.xlsx): '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not run this unless you are sure you know what you are doing and the models are a sure improvement. Running this will replace everything.\n",
        "\n",
        "!git clone https://github.com/nathanchong25/film_ml\n",
        "%cd film_ml\n",
        "!mv /content/myfile.txt /content/film_ml/\n",
        "!git config --global user.email \"nathanchong25@gmail.com\"\n",
        "!git config --global user.name \"nathanchong25\"\n",
        "\n",
        "for file in files_to_push:\n",
        "    !git add {file}\n",
        "    !git commit -m \"Added/Updated {file}\"\n",
        "\n",
        "!git push origin master"
      ],
      "metadata": {
        "id": "ygUWT0TcafYN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}